{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36057,
     "status": "ok",
     "timestamp": 1604712154209,
     "user": {
      "displayName": "余凯",
      "photoUrl": "",
      "userId": "13334016005065607071"
     },
     "user_tz": -480
    },
    "id": "BJDVi3PkXFlX",
    "outputId": "24827601-7ef1-4420-a74f-1366840546fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAS-6834hV6c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV # 网格搜索 例子：https://blog.csdn.net/ustbclearwang/article/details/81484778\n",
    "from scipy import interpolate # 插值模块 https://www.jianshu.com/p/b306095309db\n",
    "import scipy.io as sio # scipy.io(输入和输出)包用于加载和保存.mat文件的函数\n",
    "from numpy import * \n",
    "from keras.layers import Activation, multiply\n",
    "from keras.models import *\n",
    "from keras.layers.core import *\n",
    "\n",
    "datasets_path = '../data/'\n",
    "precessData_path = '../preprocess_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETgIMIeYhbvb"
   },
   "source": [
    "# data_prosessing1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmfikfVJTAXS"
   },
   "outputs": [],
   "source": [
    "def data_prosessing1(FD_id, max_RUL, winSize):\n",
    "\n",
    "    RUL = np.loadtxt(datasets_path + 'RUL_FD00{}.txt'.format(str(FD_id)))\n",
    "\n",
    "    train_raw = np.loadtxt(datasets_path + 'train_FD00{}.txt'.format(str(FD_id))) # (20631, 26)\n",
    "\n",
    "    test_raw = np.loadtxt(datasets_path + 'test_FD00{}.txt'.format(str(FD_id)))\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    train_raw[ : , 2 : ] = min_max_scaler.fit_transform(train_raw[ : , 2 : ])\n",
    "\n",
    "    test_raw[ : , 2 : ] = min_max_scaler.transform(test_raw[ : , 2 : ])\n",
    "\n",
    "    train = train_raw\n",
    "\n",
    "    test = test_raw\n",
    "\n",
    "    if FD_id == 4:\n",
    "\n",
    "        Feasize = 24\n",
    "\n",
    "    else:\n",
    "\n",
    "        train = np.delete(train, [2, 3, 4, 5, 9, 10, 14, 20, 22, 23], axis=1) \n",
    "\n",
    "        test = np.delete(test, [2, 3, 4, 5, 9, 10, 14, 20, 22, 23], axis=1) \n",
    "\n",
    "        Feasize = 14\n",
    "\n",
    "    trainX = []\n",
    "\n",
    "    trainY = []\n",
    "\n",
    "    testX = []\n",
    "\n",
    "    testY = []\n",
    "\n",
    "    ## 构造训练集数据及标签\n",
    "\n",
    "    for i in range(1, int(np.max(train[ : , 0])) + 1): # 1-101\n",
    "\n",
    "        ind = np.where(train[ : , 0] == i)[0] \n",
    "\n",
    "        data_temp = train[ind, :] # 取对应发动机的数据\n",
    "\n",
    "        for j in range(len(data_temp) - winSize + 1): \n",
    "\n",
    "            trainX.append(data_temp[j : j + winSize, 2 : ].tolist()) # 存储为二维数组\n",
    "\n",
    "            train_RUL = len(data_temp) - winSize - j \n",
    "\n",
    "            if train_RUL > max_RUL:\n",
    "\n",
    "                train_RUL = max_RUL\n",
    "\n",
    "            trainY.append(train_RUL)\n",
    "\n",
    "    ## 构造测试集数据\n",
    "\n",
    "    for i in range(1, int(np.max(test[ : , 0])) + 1): # i - the ith test dataset\n",
    "\n",
    "        ind = np.where(test[ : , 0] == i)[0] # 第i个test dataset所有的行数\n",
    "\n",
    "        data_temp = test[ind, : ] # 第i个test dataset数据放到data_temp\n",
    "\n",
    "        if len(data_temp) < winSize: # 如果数据长度不足winSize\n",
    "\n",
    "            data_temp_a = [] \n",
    "\n",
    "            for myi in range(data_temp.shape[1]): \n",
    "\n",
    "                x1 = np.linspace(0, winSize - 1, len(data_temp)) # 在 0 和 winSize-1 之间返回 len(data_temp) 个均匀间隔的数据\n",
    "\n",
    "                x_new = np.linspace(0, winSize - 1, winSize) # [0, winSize - 1]取 winSize 个数\n",
    "\n",
    "                tck = interpolate.splrep(x1, data_temp[:, myi]) # B样条表示B-spline representation  一列一列地进行离散插值\n",
    "\n",
    "                a = interpolate.splev(x_new, tck) # tck：BSpline对象 返回插值之后的每列array数据，长度为winSize\n",
    "\n",
    "                data_temp_a.append(a.tolist()) \n",
    "\n",
    "            data_temp_a = np.array(data_temp_a)\n",
    "\n",
    "            data_temp = data_temp_a.T\n",
    "\n",
    "            data_temp = data_temp[ : ,2 : ]\n",
    "\n",
    "        else:\n",
    "\n",
    "            data_temp = data_temp[-winSize : ,2 : ] \n",
    "\n",
    "        data_temp = np.reshape(data_temp, (1, data_temp.shape[0], data_temp.shape[1])) # (1, 30, 14)\n",
    "\n",
    "        if i == 1:\n",
    "\n",
    "            testX = data_temp\n",
    "\n",
    "        else:\n",
    "\n",
    "            testX = np.concatenate((testX, data_temp), axis=0)\n",
    "\n",
    "        if RUL[i - 1] > max_RUL:\n",
    "\n",
    "            testY.append(max_RUL) \n",
    "\n",
    "        else:\n",
    "\n",
    "            testY.append(RUL[i - 1])\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "\n",
    "    testX = np.array(testX)\n",
    "\n",
    "    trainY = np.array(trainY) / max_RUL # (100, )\n",
    "\n",
    "    testY = np.array(testY) / max_RUL\n",
    "\n",
    "    return trainX, trainY, testX, testY, Feasize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34Npcmg_0Km8"
   },
   "source": [
    "# 扩充时序（+2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SQ-Bp_yzXgh"
   },
   "outputs": [],
   "source": [
    "def fea_extract1(data):  # feature extraction of two features 数据增强，增加了\"线性因子\"  data为每个时序片段，大小为(30, 14, 1)\n",
    "\n",
    "    fea = [] \n",
    "\n",
    "    regr = linear_model.LinearRegression()  # feature of linear coefficient 特征值与时间轴的线性相关系数（斜率）\n",
    "\n",
    "    x = np.array(range(data.shape[0])) \n",
    "\n",
    "    for i in range(data.shape[1]):  \n",
    "\n",
    "        regr.fit(x.reshape(-1, 1), np.ravel(data[ : , i])) # data (30, 14, 1) --> data[ : , i] (30, 1)  np.ravel()将多维数组转换为一维数组 (30, 1) --> (30, ) \n",
    "\n",
    "        fea = fea + list(regr.coef_) \n",
    "\n",
    "        # 线性函数 y = w1x1 + w0 属性 coef_：权重矩阵，理解为w1； intercept_：截距，理解为w0\n",
    "\n",
    "        # 目的是为了，确定每个特征在时间轴上的值跟时间变化的线性相关性。 作为特征在新增时间步上的值，即31\n",
    "\n",
    "    return fea # 列表内共14个元素\n",
    "\n",
    "def fea_extract2(data):  # 特征在时间维度上的特征提取，data (30, 14, 1)  data[ : , i] (30, 1)\n",
    "\n",
    "    fea = []\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "\n",
    "        fea.append(np.mean(data[ : , i])) \n",
    "        \n",
    "        # 每个特征沿时序上求均值，作为特征在新增时间步上的值，即32 \n",
    "\n",
    "        # np.mean()，如果不设置axis值，则对30*1个数求均值（分母是tn - t1 = 30），返回一个\"实数\"，所以，每次循环都往fea列表中增加一个\"实数\"，共14个元素\n",
    "\n",
    "    return fea\n",
    "\n",
    "def data_prosessing2(trainX, testX, winSize, Feasize):\n",
    "\n",
    "    trainX = np.reshape(trainX, [trainX.shape[0], winSize, Feasize, 1]) # trainX (?, 30, 14) --> (?, 30, 14, 1)\n",
    "\n",
    "    testX = np.reshape(testX, [testX.shape[0], winSize, Feasize, 1]) # (100, 30, 14) --> (100, 30, 14, 1)\n",
    "\n",
    "    trainX_fea1 = [] \n",
    "\n",
    "    testX_fea1 = []\n",
    "\n",
    "    trainX_fea2 = []\n",
    "\n",
    "    testX_fea2 = []\n",
    "\n",
    "    for i in range(len(trainX)):\n",
    "\n",
    "        data_temp = trainX[i] # (30, 14, 1)\n",
    "\n",
    "        trainX_fea1.append(fea_extract1(data_temp)) # 列表内有17731个列表，每个列表元素为14，类似于二维数组（17731，14）\n",
    "\n",
    "        trainX_fea2.append(fea_extract2(data_temp)) \n",
    "\n",
    "    for i in range(len(testX)):\n",
    "\n",
    "        data_temp = testX[i] # (30, 14, 1)\n",
    "\n",
    "        testX_fea1.append(fea_extract1(data_temp)) # 列表内有100个列表，每个列表元素为14，类似于二维数组（100，14）\n",
    "\n",
    "        testX_fea2.append(fea_extract2(data_temp))\n",
    "\n",
    "    scale1 = preprocessing.MinMaxScaler().fit(trainX_fea1)\n",
    "\n",
    "    trainX_fea1 = scale1.transform(trainX_fea1) # 归一化训练集\n",
    "\n",
    "    testX_fea1 = scale1.transform(testX_fea1) # 归一化测试集\n",
    "\n",
    "    scale2 = preprocessing.MinMaxScaler().fit(trainX_fea2)\n",
    "\n",
    "    trainX_fea2 = scale2.transform(trainX_fea2)\n",
    "\n",
    "    testX_fea2 = scale2.transform(testX_fea2)\n",
    "\n",
    "    trainX_new = []\n",
    "\n",
    "    testX_new = []\n",
    "\n",
    "    for i in range(len(trainX)):\n",
    "\n",
    "        data_temp0 = trainX[i] # (30, 14, 1)\n",
    "\n",
    "        data_temp1 = np.reshape(trainX_fea1[i], [1, Feasize, 1])  # trainX_fea1为二维列表，trainX_fea1[i]为含有14个元素的list，重塑为（1， 14， 1）\n",
    "\n",
    "        data_temp2 = np.reshape(trainX_fea2[i], [1, Feasize, 1])\n",
    "\n",
    "        data_temp = np.vstack((data_temp0, data_temp1, data_temp2)) # (30 + 1 + 1, 14, 1) = (32, 14, 1)\n",
    "\n",
    "        trainX_new.append(data_temp)\n",
    "\n",
    "    trainX_new = np.array(trainX_new) # list转化为数组\n",
    "\n",
    "    for i in range(len(testX)):\n",
    "\n",
    "        data_temp0 = testX[i]\n",
    "\n",
    "        data_temp1 = np.reshape(testX_fea1[i], [1, Feasize, 1])  \n",
    "\n",
    "        data_temp2 = np.reshape(testX_fea2[i], [1, Feasize, 1])\n",
    "\n",
    "        data_temp = np.vstack((data_temp0, data_temp1, data_temp2))\n",
    "\n",
    "        testX_new.append(data_temp)\n",
    "\n",
    "    testX_new = np.array(testX_new) \n",
    "\n",
    "    winSize_new = trainX_new.shape[1]\n",
    "\n",
    "    return trainX_new, testX_new, winSize_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNHRwc_vvWsT"
   },
   "source": [
    "# 保存片段数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xS1x2ukmu0P6"
   },
   "outputs": [],
   "source": [
    "def save_ori_data(FD_id, winSize, trainX, trainY, testX, testY):\n",
    "  sio.savemat(precessData_path + 'trainX{}_{}.mat'.format(str(FD_id), str(winSize)), {'trainX{}'.format(str(FD_id)): trainX})\n",
    "  sio.savemat(precessData_path + 'trainY{}.mat'.format(str(FD_id)), {'trainY{}'.format(str(FD_id)): trainY}) # trainY (17731,) sio.loadmat之后变成二维数组 (1, 17731)\n",
    "  sio.savemat(precessData_path + 'testX{}_{}.mat'.format(str(FD_id), str(winSize)), {'testX{}'.format(str(FD_id)): testX})\n",
    "  sio.savemat(precessData_path + 'testY{}.mat'.format(str(FD_id)), {'testY{}'.format(str(FD_id)): testY})\n",
    "\n",
    "def save_extend_data(FD_id, winSize, trainX, testX):\n",
    "  sio.savemat(precessData_path + 'trainX{}_{}.mat'.format(str(FD_id), str(winSize)), {'trainX{}'.format(str(FD_id)): trainX})\n",
    "  sio.savemat(precessData_path + 'testX{}_{}.mat'.format(str(FD_id), str(winSize)), {'testX{}'.format(str(FD_id)): testX})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqcEA1WGxktp"
   },
   "source": [
    "# 处理四个子数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QNxCbw3qCMi"
   },
   "outputs": [],
   "source": [
    "FD_1 = 1 # 剔除 2, 3, 4, 5, 9, 10, 14, 20, 22, 23 + （26， 27）列 14个特征\n",
    "FD_2 = 2 # 保留全部列 14 + 10 = 24\n",
    "FD_3 = 3 # 剔除 (2, 3) 4, 5, 9, 10, 14, 20, 22, 23 + （26， 27）列 14 \\ 16个特征\n",
    "FD_4 = 4 # 保留全部列 14 + 10 = 24\n",
    "\n",
    "max_RUL = 125.0  # max RUL for training\n",
    "\n",
    "winSize_1 = 30 # FD001: 30(31)\n",
    "winSize_2 = 20 # FD002: 20(21)\n",
    "winSize_3 = 35 # FD003: 35(38)\n",
    "winSize_4 = 19 # FD004: 19(19)\n",
    "\n",
    "# 原始片段数据\n",
    "trainX_1, trainY_1, testX_1, testY_1, Feasize_1 = data_prosessing1(FD_1, max_RUL, winSize_1)\n",
    "trainX_2, trainY_2, testX_2, testY_2, Feasize_2 = data_prosessing1(FD_2, max_RUL, winSize_2)\n",
    "trainX_3, trainY_3, testX_3, testY_3, Feasize_3 = data_prosessing1(FD_3, max_RUL, winSize_3)\n",
    "trainX_4, trainY_4, testX_4, testY_4, Feasize_4 = data_prosessing1(FD_4, max_RUL, winSize_4)\n",
    "\n",
    "# 保存片段化的原始数据\n",
    "save_ori_data(FD_1, winSize_1, trainX_1, trainY_1, testX_1, testY_1)\n",
    "save_ori_data(FD_2, winSize_2, trainX_2, trainY_2, testX_2, testY_2)\n",
    "save_ori_data(FD_3, winSize_3, trainX_3, trainY_3, testX_3, testY_3)\n",
    "save_ori_data(FD_4, winSize_4, trainX_4, trainY_4, testX_4, testY_4)\n",
    "\n",
    "# 时序扩充数据\n",
    "trainX_1_new, testX_1_new, winSize_1_new = data_prosessing2(trainX_1, testX_1, winSize_1, Feasize_1)\n",
    "trainX_2_new, testX_2_new, winSize_2_new = data_prosessing2(trainX_2, testX_2, winSize_2, Feasize_2)\n",
    "trainX_3_new, testX_3_new, winSize_3_new = data_prosessing2(trainX_3, testX_3, winSize_3, Feasize_3)\n",
    "trainX_4_new, testX_4_new, winSize_4_new = data_prosessing2(trainX_4, testX_4, winSize_4, Feasize_4)\n",
    "\n",
    "# 保存时序扩充数据\n",
    "save_extend_data(FD_1, winSize_1_new, trainX_1_new, testX_1_new)\n",
    "save_extend_data(FD_2, winSize_2_new, trainX_2_new, testX_2_new)\n",
    "save_extend_data(FD_3, winSize_3_new, trainX_3_new, testX_3_new)\n",
    "save_extend_data(FD_4, winSize_4_new, trainX_4_new, testX_4_new)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPFp/yIUJxVzgdqCiczjfmm",
   "collapsed_sections": [],
   "name": "dataprocessingTCN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
